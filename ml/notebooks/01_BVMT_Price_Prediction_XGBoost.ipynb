{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ¦ BVMT Stock Price Prediction with XGBoost\n",
    "## IHEC-CODELAB 2.0 - Complete Production-Ready Pipeline\n",
    "\n",
    "**Objective:** Predict the next 5 days closing prices for BVMT stocks\n",
    "\n",
    "**Model:** XGBoost (fast, accurate, interpretable)\n",
    "\n",
    "**Metrics:** RMSE, MAE, Directional Accuracy (most important for trading)\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“‹ Table of Contents\n",
    "1. Setup & Data Loading\n",
    "2. Data Exploration & Cleaning\n",
    "3. Feature Engineering\n",
    "4. Model Training with Optimized Hyperparameters\n",
    "5. Evaluation & Visualization\n",
    "6. Export for Production"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ðŸ“¦ Setup & Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run in Colab)\n",
    "!pip install -q xgboost pandas numpy scikit-learn matplotlib seaborn ta joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('dark_background')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "print(\"âœ… Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“ Upload your data files to Colab\n",
    "# Run this cell and upload all histo_cotation files\n",
    "from google.colab import files\n",
    "uploaded = files.upload()\n",
    "print(f\"âœ… Uploaded {len(uploaded)} files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_txt_file(filepath):\n",
    "    \"\"\"\n",
    "    Load 2016-2021 fixed-width text files.\n",
    "    These have a different format than the CSV files.\n",
    "    \"\"\"\n",
    "    # Read with fixed width format\n",
    "    df = pd.read_fwf(\n",
    "        filepath,\n",
    "        skiprows=2,  # Skip header rows\n",
    "        header=None,\n",
    "        names=['SEANCE', 'GROUPE', 'CODE', 'VALEUR', 'OUVERTURE', 'CLOTURE', \n",
    "               'PLUS_BAS', 'PLUS_HAUT', 'QUANTITE_NEGOCIEE', 'NB_TRANSACTION', 'CAPITAUX', 'IND_RES'],\n",
    "        encoding='latin-1'\n",
    "    )\n",
    "    # Drop the IND_RES column if it exists\n",
    "    if 'IND_RES' in df.columns:\n",
    "        df = df.drop('IND_RES', axis=1)\n",
    "    return df\n",
    "\n",
    "def load_csv_file(filepath):\n",
    "    \"\"\"\n",
    "    Load 2022-2025 CSV files (semicolon-separated).\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(filepath, sep=';', encoding='utf-8')\n",
    "    # Clean column names\n",
    "    df.columns = df.columns.str.strip()\n",
    "    return df\n",
    "\n",
    "def load_all_data():\n",
    "    \"\"\"\n",
    "    Load all available data files and combine them.\n",
    "    \"\"\"\n",
    "    all_dfs = []\n",
    "    \n",
    "    # Load TXT files (2016-2021)\n",
    "    for year in range(2016, 2022):\n",
    "        try:\n",
    "            df = load_txt_file(f'histo_cotation_{year}.txt')\n",
    "            df['source_year'] = year\n",
    "            all_dfs.append(df)\n",
    "            print(f\"âœ… Loaded {year}: {len(df)} rows\")\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Could not load {year}: {e}\")\n",
    "    \n",
    "    # Load CSV files (2022-2025)\n",
    "    for year in range(2022, 2026):\n",
    "        try:\n",
    "            df = load_csv_file(f'histo_cotation_{year}.csv')\n",
    "            df['source_year'] = year\n",
    "            all_dfs.append(df)\n",
    "            print(f\"âœ… Loaded {year}: {len(df)} rows\")\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Could not load {year}: {e}\")\n",
    "    \n",
    "    # Combine all dataframes\n",
    "    combined = pd.concat(all_dfs, ignore_index=True)\n",
    "    print(f\"\\nðŸ“Š Total combined: {len(combined)} rows\")\n",
    "    return combined\n",
    "\n",
    "# Load all data\n",
    "raw_data = load_all_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ðŸ” Data Exploration & Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic info\n",
    "print(\"ðŸ“‹ Dataset Info:\")\n",
    "print(f\"Shape: {raw_data.shape}\")\n",
    "print(f\"\\nColumns: {raw_data.columns.tolist()}\")\n",
    "print(f\"\\nData types:\\n{raw_data.dtypes}\")\n",
    "print(f\"\\nSample data:\")\n",
    "raw_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    \"\"\"\n",
    "    Clean and standardize the BVMT dataset.\n",
    "    \n",
    "    Steps:\n",
    "    1. Parse dates\n",
    "    2. Filter to main market (GROUPE=11)\n",
    "    3. Convert numeric columns\n",
    "    4. Handle missing values\n",
    "    5. Remove zero-volume days (no trading)\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # 1. Parse date - handle multiple formats\n",
    "    def parse_date(x):\n",
    "        if pd.isna(x):\n",
    "            return pd.NaT\n",
    "        x = str(x).strip()\n",
    "        for fmt in ['%d/%m/%Y', '%Y-%m-%d', '%d-%m-%Y']:\n",
    "            try:\n",
    "                return pd.to_datetime(x, format=fmt)\n",
    "            except:\n",
    "                continue\n",
    "        return pd.NaT\n",
    "    \n",
    "    df['date'] = df['SEANCE'].apply(parse_date)\n",
    "    \n",
    "    # 2. Filter to main market stocks (GROUPE=11)\n",
    "    df['GROUPE'] = pd.to_numeric(df['GROUPE'], errors='coerce')\n",
    "    df = df[df['GROUPE'] == 11].copy()\n",
    "    print(f\"âœ… Filtered to main market: {len(df)} rows\")\n",
    "    \n",
    "    # 3. Convert numeric columns\n",
    "    numeric_cols = ['OUVERTURE', 'CLOTURE', 'PLUS_BAS', 'PLUS_HAUT', \n",
    "                    'QUANTITE_NEGOCIEE', 'NB_TRANSACTION', 'CAPITAUX']\n",
    "    for col in numeric_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    \n",
    "    # Rename columns to English\n",
    "    df = df.rename(columns={\n",
    "        'VALEUR': 'stock_name',\n",
    "        'CODE': 'stock_code',\n",
    "        'OUVERTURE': 'open',\n",
    "        'CLOTURE': 'close',\n",
    "        'PLUS_BAS': 'low',\n",
    "        'PLUS_HAUT': 'high',\n",
    "        'QUANTITE_NEGOCIEE': 'volume',\n",
    "        'NB_TRANSACTION': 'transactions',\n",
    "        'CAPITAUX': 'capital'\n",
    "    })\n",
    "    \n",
    "    # 4. Handle missing/invalid values\n",
    "    # Remove rows with NaN dates\n",
    "    df = df.dropna(subset=['date'])\n",
    "    \n",
    "    # Remove rows with zero or NaN close price\n",
    "    df = df[(df['close'] > 0) & df['close'].notna()]\n",
    "    \n",
    "    # 5. Remove zero-volume days (no actual trading)\n",
    "    df = df[df['volume'] > 0].copy()\n",
    "    print(f\"âœ… After removing zero-volume days: {len(df)} rows\")\n",
    "    \n",
    "    # Sort by stock and date\n",
    "    df = df.sort_values(['stock_name', 'date']).reset_index(drop=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Clean the data\n",
    "data = clean_data(raw_data)\n",
    "print(f\"\\nðŸ“Š Cleaned dataset shape: {data.shape}\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze stocks - focus on most liquid ones\n",
    "stock_stats = data.groupby('stock_name').agg({\n",
    "    'volume': ['count', 'mean', 'sum'],\n",
    "    'close': ['mean', 'std'],\n",
    "    'date': ['min', 'max']\n",
    "}).round(2)\n",
    "\n",
    "stock_stats.columns = ['trading_days', 'avg_volume', 'total_volume', 'avg_price', 'price_std', 'first_date', 'last_date']\n",
    "stock_stats = stock_stats.sort_values('total_volume', ascending=False)\n",
    "\n",
    "print(\"ðŸ† Top 15 Most Liquid Stocks (by total volume):\")\n",
    "top_stocks = stock_stats.head(15)\n",
    "display(top_stocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select top liquid stocks for modeling\n",
    "TOP_N_STOCKS = 15\n",
    "selected_stocks = stock_stats.head(TOP_N_STOCKS).index.tolist()\n",
    "\n",
    "# Filter data to selected stocks\n",
    "data = data[data['stock_name'].isin(selected_stocks)].copy()\n",
    "print(f\"\\nðŸ“Š Selected {TOP_N_STOCKS} stocks with {len(data)} total trading records\")\n",
    "print(f\"\\nSelected stocks: {selected_stocks}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize price history for top stocks\n",
    "fig, axes = plt.subplots(3, 2, figsize=(16, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, stock in enumerate(selected_stocks[:6]):\n",
    "    stock_data = data[data['stock_name'] == stock]\n",
    "    axes[i].plot(stock_data['date'], stock_data['close'], color='#ff4444', linewidth=1)\n",
    "    axes[i].set_title(f'{stock}', fontsize=12, color='white')\n",
    "    axes[i].set_xlabel('Date')\n",
    "    axes[i].set_ylabel('Close Price (TND)')\n",
    "    axes[i].grid(alpha=0.3)\n",
    "\n",
    "plt.suptitle('BVMT Top Stocks - Price History', fontsize=16, color='white', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ðŸ› ï¸ Feature Engineering\n",
    "\n",
    "This is the **most critical step** for XGBoost performance.\n",
    "\n",
    "We'll create:\n",
    "- **Price features**: Returns, volatility, moving averages\n",
    "- **Volume features**: Volume ratios, trends\n",
    "- **Technical indicators**: RSI, MACD, Bollinger Bands\n",
    "- **Time features**: Day of week, month effects\n",
    "- **Lagged features**: Past N days prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(df, lookback=60):\n",
    "    \"\"\"\n",
    "    Create comprehensive features for stock prediction.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with OHLCV data for a single stock\n",
    "        lookback: Number of past days to use for features\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with all features\n",
    "    \"\"\"\n",
    "    df = df.copy().sort_values('date').reset_index(drop=True)\n",
    "    \n",
    "    # ===== PRICE FEATURES =====\n",
    "    \n",
    "    # Returns (log returns for stability)\n",
    "    df['return_1d'] = np.log(df['close'] / df['close'].shift(1))\n",
    "    df['return_5d'] = np.log(df['close'] / df['close'].shift(5))\n",
    "    df['return_10d'] = np.log(df['close'] / df['close'].shift(10))\n",
    "    df['return_20d'] = np.log(df['close'] / df['close'].shift(20))\n",
    "    \n",
    "    # Volatility (rolling std of returns)\n",
    "    df['volatility_5d'] = df['return_1d'].rolling(5).std()\n",
    "    df['volatility_10d'] = df['return_1d'].rolling(10).std()\n",
    "    df['volatility_20d'] = df['return_1d'].rolling(20).std()\n",
    "    \n",
    "    # Moving Averages\n",
    "    for window in [5, 10, 20, 50]:\n",
    "        df[f'sma_{window}'] = df['close'].rolling(window).mean()\n",
    "        df[f'price_to_sma_{window}'] = df['close'] / df[f'sma_{window}']\n",
    "    \n",
    "    # Exponential Moving Averages\n",
    "    df['ema_12'] = df['close'].ewm(span=12).mean()\n",
    "    df['ema_26'] = df['close'].ewm(span=26).mean()\n",
    "    df['price_to_ema_12'] = df['close'] / df['ema_12']\n",
    "    \n",
    "    # ===== VOLUME FEATURES =====\n",
    "    \n",
    "    df['volume_sma_5'] = df['volume'].rolling(5).mean()\n",
    "    df['volume_sma_20'] = df['volume'].rolling(20).mean()\n",
    "    df['volume_ratio'] = df['volume'] / df['volume_sma_20']\n",
    "    df['volume_change'] = df['volume'].pct_change()\n",
    "    \n",
    "    # ===== TECHNICAL INDICATORS =====\n",
    "    \n",
    "    # RSI (Relative Strength Index)\n",
    "    delta = df['close'].diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
    "    rs = gain / loss\n",
    "    df['rsi_14'] = 100 - (100 / (1 + rs))\n",
    "    \n",
    "    # MACD\n",
    "    df['macd'] = df['ema_12'] - df['ema_26']\n",
    "    df['macd_signal'] = df['macd'].ewm(span=9).mean()\n",
    "    df['macd_hist'] = df['macd'] - df['macd_signal']\n",
    "    \n",
    "    # Bollinger Bands\n",
    "    df['bb_middle'] = df['close'].rolling(20).mean()\n",
    "    df['bb_std'] = df['close'].rolling(20).std()\n",
    "    df['bb_upper'] = df['bb_middle'] + 2 * df['bb_std']\n",
    "    df['bb_lower'] = df['bb_middle'] - 2 * df['bb_std']\n",
    "    df['bb_width'] = (df['bb_upper'] - df['bb_lower']) / df['bb_middle']\n",
    "    df['bb_position'] = (df['close'] - df['bb_lower']) / (df['bb_upper'] - df['bb_lower'])\n",
    "    \n",
    "    # Price Range Features\n",
    "    df['intraday_range'] = (df['high'] - df['low']) / df['close']\n",
    "    df['gap_open'] = (df['open'] - df['close'].shift(1)) / df['close'].shift(1)\n",
    "    \n",
    "    # ===== TIME FEATURES =====\n",
    "    \n",
    "    df['day_of_week'] = df['date'].dt.dayofweek\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['quarter'] = df['date'].dt.quarter\n",
    "    df['is_month_start'] = df['date'].dt.is_month_start.astype(int)\n",
    "    df['is_month_end'] = df['date'].dt.is_month_end.astype(int)\n",
    "    \n",
    "    # ===== LAGGED FEATURES =====\n",
    "    \n",
    "    # Past N days closing prices (normalized as returns)\n",
    "    for lag in [1, 2, 3, 5, 10, 20]:\n",
    "        df[f'lag_{lag}d_return'] = np.log(df['close'] / df['close'].shift(lag))\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply feature engineering to each stock\n",
    "print(\"ðŸ› ï¸ Creating features for each stock...\")\n",
    "featured_data = []\n",
    "\n",
    "for stock in selected_stocks:\n",
    "    stock_df = data[data['stock_name'] == stock].copy()\n",
    "    stock_df = create_features(stock_df)\n",
    "    featured_data.append(stock_df)\n",
    "    print(f\"  âœ… {stock}: {len(stock_df)} rows\")\n",
    "\n",
    "data_with_features = pd.concat(featured_data, ignore_index=True)\n",
    "print(f\"\\nðŸ“Š Total feature-enriched data: {data_with_features.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display feature columns\n",
    "feature_cols = [col for col in data_with_features.columns if col not in \n",
    "                ['date', 'stock_name', 'stock_code', 'GROUPE', 'source_year', \n",
    "                 'open', 'high', 'low', 'close', 'volume', 'transactions', 'capital']]\n",
    "\n",
    "print(f\"ðŸ“‹ Created {len(feature_cols)} features:\")\n",
    "for i, col in enumerate(feature_cols, 1):\n",
    "    print(f\"  {i}. {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ðŸŽ¯ Create Target Variables (5-Day Prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_targets(df, horizons=[1, 2, 3, 4, 5]):\n",
    "    \"\"\"\n",
    "    Create target variables for multi-horizon prediction.\n",
    "    \n",
    "    We predict:\n",
    "    - Future returns (for direction)\n",
    "    - Future prices (for actual value)\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    for h in horizons:\n",
    "        # Target: Price return in h days\n",
    "        df[f'target_return_{h}d'] = (df['close'].shift(-h) - df['close']) / df['close']\n",
    "        \n",
    "        # Target: Actual future price\n",
    "        df[f'target_price_{h}d'] = df['close'].shift(-h)\n",
    "        \n",
    "        # Target: Direction (1=up, 0=down)\n",
    "        df[f'target_direction_{h}d'] = (df['close'].shift(-h) > df['close']).astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply to each stock separately\n",
    "print(\"ðŸŽ¯ Creating target variables...\")\n",
    "final_data = []\n",
    "\n",
    "for stock in selected_stocks:\n",
    "    stock_df = data_with_features[data_with_features['stock_name'] == stock].copy()\n",
    "    stock_df = create_targets(stock_df)\n",
    "    final_data.append(stock_df)\n",
    "\n",
    "df_final = pd.concat(final_data, ignore_index=True)\n",
    "\n",
    "# Drop rows with NaN features or targets\n",
    "initial_rows = len(df_final)\n",
    "df_final = df_final.dropna()\n",
    "print(f\"âœ… Dropped {initial_rows - len(df_final)} rows with NaN values\")\n",
    "print(f\"ðŸ“Š Final dataset: {df_final.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature columns for training\n",
    "FEATURE_COLS = [\n",
    "    # Returns\n",
    "    'return_1d', 'return_5d', 'return_10d', 'return_20d',\n",
    "    # Volatility\n",
    "    'volatility_5d', 'volatility_10d', 'volatility_20d',\n",
    "    # Moving Averages\n",
    "    'price_to_sma_5', 'price_to_sma_10', 'price_to_sma_20', 'price_to_sma_50',\n",
    "    'price_to_ema_12',\n",
    "    # Volume\n",
    "    'volume_ratio', 'volume_change',\n",
    "    # Technical Indicators\n",
    "    'rsi_14', 'macd', 'macd_signal', 'macd_hist',\n",
    "    'bb_width', 'bb_position',\n",
    "    # Price Range\n",
    "    'intraday_range', 'gap_open',\n",
    "    # Time\n",
    "    'day_of_week', 'month', 'quarter', 'is_month_start', 'is_month_end',\n",
    "    # Lagged Returns\n",
    "    'lag_1d_return', 'lag_2d_return', 'lag_3d_return', 'lag_5d_return', 'lag_10d_return', 'lag_20d_return'\n",
    "]\n",
    "\n",
    "# Verify all features exist\n",
    "missing_features = [f for f in FEATURE_COLS if f not in df_final.columns]\n",
    "if missing_features:\n",
    "    print(f\"âš ï¸ Missing features: {missing_features}\")\n",
    "else:\n",
    "    print(f\"âœ… All {len(FEATURE_COLS)} features available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ðŸš‚ Model Training with Optimized Hyperparameters\n",
    "\n",
    "### XGBoost Hyperparameters (Optimized for Stock Prediction)\n",
    "\n",
    "Based on winning solutions from Kaggle stock prediction competitions and research papers:\n",
    "\n",
    "- **n_estimators**: 500-1000 (with early stopping)\n",
    "- **max_depth**: 6-8 (prevent overfitting on noisy data)\n",
    "- **learning_rate**: 0.01-0.05 (slow and steady)\n",
    "- **subsample**: 0.8 (bootstrap samples)\n",
    "- **colsample_bytree**: 0.8 (feature sampling)\n",
    "- **min_child_weight**: 5-10 (regularization)\n",
    "- **reg_alpha/reg_lambda**: L1/L2 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import xgboost as xgb\n",
    "\n",
    "# Prepare training data\n",
    "# Use temporal split: Train on older data, test on recent data\n",
    "\n",
    "# Sort by date\n",
    "df_sorted = df_final.sort_values('date').reset_index(drop=True)\n",
    "\n",
    "# Split: 80% train, 10% validation, 10% test (temporal)\n",
    "n = len(df_sorted)\n",
    "train_end = int(n * 0.8)\n",
    "val_end = int(n * 0.9)\n",
    "\n",
    "train_df = df_sorted.iloc[:train_end]\n",
    "val_df = df_sorted.iloc[train_end:val_end]\n",
    "test_df = df_sorted.iloc[val_end:]\n",
    "\n",
    "print(f\"ðŸ“Š Data Split:\")\n",
    "print(f\"  Train: {len(train_df)} rows ({train_df['date'].min()} to {train_df['date'].max()})\")\n",
    "print(f\"  Val:   {len(val_df)} rows ({val_df['date'].min()} to {val_df['date'].max()})\")\n",
    "print(f\"  Test:  {len(test_df)} rows ({test_df['date'].min()} to {test_df['date'].max()})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and targets\n",
    "X_train = train_df[FEATURE_COLS].values\n",
    "X_val = val_df[FEATURE_COLS].values\n",
    "X_test = test_df[FEATURE_COLS].values\n",
    "\n",
    "# We'll train separate models for each prediction horizon (1-5 days)\n",
    "# For hackathon, we'll focus on predicting returns and convert to prices\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"âœ… Features scaled\")\n",
    "print(f\"  X_train: {X_train_scaled.shape}\")\n",
    "print(f\"  X_val: {X_val_scaled.shape}\")\n",
    "print(f\"  X_test: {X_test_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimized XGBoost parameters for stock prediction\n",
    "XGB_PARAMS = {\n",
    "    'objective': 'reg:squarederror',\n",
    "    'n_estimators': 1000,\n",
    "    'max_depth': 6,\n",
    "    'learning_rate': 0.03,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'min_child_weight': 5,\n",
    "    'reg_alpha': 0.1,       # L1 regularization\n",
    "    'reg_lambda': 1.0,      # L2 regularization\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1,\n",
    "    'early_stopping_rounds': 50,\n",
    "}\n",
    "\n",
    "print(\"ðŸŽ›ï¸ XGBoost Hyperparameters:\")\n",
    "for k, v in XGB_PARAMS.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models for each prediction horizon\n",
    "HORIZONS = [1, 2, 3, 4, 5]\n",
    "models = {}\n",
    "results = {}\n",
    "\n",
    "for horizon in HORIZONS:\n",
    "    print(f\"\\nðŸš‚ Training model for {horizon}-day prediction...\")\n",
    "    \n",
    "    # Get targets\n",
    "    y_train = train_df[f'target_return_{horizon}d'].values\n",
    "    y_val = val_df[f'target_return_{horizon}d'].values\n",
    "    y_test = test_df[f'target_return_{horizon}d'].values\n",
    "    \n",
    "    # Create and train model\n",
    "    model = xgb.XGBRegressor(**XGB_PARAMS)\n",
    "    \n",
    "    model.fit(\n",
    "        X_train_scaled, y_train,\n",
    "        eval_set=[(X_val_scaled, y_val)],\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred_train = model.predict(X_train_scaled)\n",
    "    y_pred_val = model.predict(X_val_scaled)\n",
    "    y_pred_test = model.predict(X_test_scaled)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "    val_rmse = np.sqrt(mean_squared_error(y_val, y_pred_val))\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "    \n",
    "    train_mae = mean_absolute_error(y_train, y_pred_train)\n",
    "    val_mae = mean_absolute_error(y_val, y_pred_val)\n",
    "    test_mae = mean_absolute_error(y_test, y_pred_test)\n",
    "    \n",
    "    # Directional accuracy (most important for trading!)\n",
    "    train_dir_acc = np.mean((y_pred_train > 0) == (y_train > 0))\n",
    "    val_dir_acc = np.mean((y_pred_val > 0) == (y_val > 0))\n",
    "    test_dir_acc = np.mean((y_pred_test > 0) == (y_test > 0))\n",
    "    \n",
    "    # Store model and results\n",
    "    models[horizon] = model\n",
    "    results[horizon] = {\n",
    "        'train_rmse': train_rmse, 'val_rmse': val_rmse, 'test_rmse': test_rmse,\n",
    "        'train_mae': train_mae, 'val_mae': val_mae, 'test_mae': test_mae,\n",
    "        'train_dir_acc': train_dir_acc, 'val_dir_acc': val_dir_acc, 'test_dir_acc': test_dir_acc,\n",
    "        'best_iteration': model.best_iteration if hasattr(model, 'best_iteration') else XGB_PARAMS['n_estimators']\n",
    "    }\n",
    "    \n",
    "    print(f\"  âœ… {horizon}d Model - Test RMSE: {test_rmse:.6f}, Test MAE: {test_mae:.6f}\")\n",
    "    print(f\"     ðŸ“ˆ Directional Accuracy: Train={train_dir_acc:.2%}, Val={val_dir_acc:.2%}, Test={test_dir_acc:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results summary\n",
    "results_df = pd.DataFrame(results).T\n",
    "results_df.index.name = 'Horizon (days)'\n",
    "\n",
    "print(\"\\nðŸ“Š Model Performance Summary:\")\n",
    "print(\"=\"*80)\n",
    "print(results_df.round(4).to_string())\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Calculate average directional accuracy\n",
    "avg_dir_acc = results_df['test_dir_acc'].mean()\n",
    "print(f\"\\nðŸŽ¯ Average Directional Accuracy: {avg_dir_acc:.2%}\")\n",
    "print(\"(Anything above 50% is profitable in theory!)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance Analysis\n",
    "print(\"\\nðŸ“Š Feature Importance (1-day model):\")\n",
    "\n",
    "importance = pd.DataFrame({\n",
    "    'feature': FEATURE_COLS,\n",
    "    'importance': models[1].feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Plot top 15 features\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "top_features = importance.head(15)\n",
    "ax.barh(top_features['feature'], top_features['importance'], color='#ff4444')\n",
    "ax.set_xlabel('Importance')\n",
    "ax.set_title('Top 15 Most Important Features', color='white')\n",
    "ax.invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(importance.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ðŸ“ˆ Prediction Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions for a sample stock\n",
    "sample_stock = 'SFBT'\n",
    "sample_data = test_df[test_df['stock_name'] == sample_stock].copy()\n",
    "\n",
    "if len(sample_data) > 0:\n",
    "    # Get features for this stock\n",
    "    X_sample = scaler.transform(sample_data[FEATURE_COLS].values)\n",
    "    \n",
    "    # Predict returns and convert to prices\n",
    "    for horizon in HORIZONS:\n",
    "        pred_returns = models[horizon].predict(X_sample)\n",
    "        sample_data[f'pred_price_{horizon}d'] = sample_data['close'] * (1 + pred_returns)\n",
    "    \n",
    "    # Plot actual vs predicted\n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "    \n",
    "    ax.plot(sample_data['date'], sample_data['close'], \n",
    "            label='Actual Price', color='white', linewidth=2)\n",
    "    \n",
    "    ax.plot(sample_data['date'], sample_data[f'target_price_5d'].shift(5), \n",
    "            label='Actual 5-day Forward', color='#00ff00', linewidth=1, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    ax.plot(sample_data['date'], sample_data['pred_price_5d'], \n",
    "            label='Predicted 5-day Forward', color='#ff4444', linewidth=1, linestyle='--')\n",
    "    \n",
    "    ax.set_title(f'{sample_stock} - Price Prediction (Test Set)', color='white', fontsize=14)\n",
    "    ax.set_xlabel('Date')\n",
    "    ax.set_ylabel('Price (TND)')\n",
    "    ax.legend()\n",
    "    ax.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(f\"âš ï¸ No test data for {sample_stock}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ðŸ’¾ Export Models for Production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import json\n",
    "\n",
    "# Create models directory\n",
    "import os\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "# Save all models\n",
    "for horizon, model in models.items():\n",
    "    model.save_model(f'models/price_predictor_{horizon}d.json')\n",
    "    print(f\"âœ… Saved models/price_predictor_{horizon}d.json\")\n",
    "\n",
    "# Save scaler\n",
    "joblib.dump(scaler, 'models/feature_scaler.pkl')\n",
    "print(\"âœ… Saved models/feature_scaler.pkl\")\n",
    "\n",
    "# Save configuration\n",
    "config = {\n",
    "    'feature_columns': FEATURE_COLS,\n",
    "    'prediction_horizons': HORIZONS,\n",
    "    'selected_stocks': selected_stocks,\n",
    "    'model_params': {k: v for k, v in XGB_PARAMS.items() if k != 'early_stopping_rounds'},\n",
    "    'results': {str(k): {kk: float(vv) for kk, vv in v.items()} for k, v in results.items()},\n",
    "    'created_at': datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "with open('models/config.json', 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "print(\"âœ… Saved models/config.json\")\n",
    "\n",
    "print(\"\\nðŸ“¦ All models exported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download models (for Colab)\n",
    "from google.colab import files\n",
    "\n",
    "# Create a zip of all models\n",
    "!zip -r models.zip models/\n",
    "files.download('models.zip')\n",
    "\n",
    "print(\"\\nðŸ“¥ Download complete! Extract models.zip to your project's ml/models/ folder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ðŸ”§ Inference Function (for Backend Integration)\n",
    "\n",
    "Copy this function to your backend service!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === COPY THIS TO YOUR BACKEND ===\n",
    "\n",
    "class BVMTPricePredictor:\n",
    "    \"\"\"\n",
    "    Production-ready price prediction service for BVMT stocks.\n",
    "    \n",
    "    Usage:\n",
    "        predictor = BVMTPricePredictor('models/')\n",
    "        prediction = predictor.predict('SFBT', current_features)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_dir: str):\n",
    "        self.model_dir = model_dir\n",
    "        self.models = {}\n",
    "        self.scaler = None\n",
    "        self.config = None\n",
    "        self._load_models()\n",
    "    \n",
    "    def _load_models(self):\n",
    "        \"\"\"Load all models and config\"\"\"\n",
    "        import json\n",
    "        import joblib\n",
    "        import xgboost as xgb\n",
    "        \n",
    "        # Load config\n",
    "        with open(f'{self.model_dir}/config.json', 'r') as f:\n",
    "            self.config = json.load(f)\n",
    "        \n",
    "        # Load scaler\n",
    "        self.scaler = joblib.load(f'{self.model_dir}/feature_scaler.pkl')\n",
    "        \n",
    "        # Load models for each horizon\n",
    "        for horizon in self.config['prediction_horizons']:\n",
    "            model = xgb.XGBRegressor()\n",
    "            model.load_model(f'{self.model_dir}/price_predictor_{horizon}d.json')\n",
    "            self.models[horizon] = model\n",
    "        \n",
    "        print(f\"âœ… Loaded {len(self.models)} models for horizons: {list(self.models.keys())}\")\n",
    "    \n",
    "    def predict(self, stock_name: str, features: dict) -> dict:\n",
    "        \"\"\"\n",
    "        Predict prices for next 5 days.\n",
    "        \n",
    "        Args:\n",
    "            stock_name: Name of the stock (e.g., 'SFBT')\n",
    "            features: Dictionary of feature values\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with predictions for each horizon\n",
    "        \"\"\"\n",
    "        import numpy as np\n",
    "        \n",
    "        # Get current price (must be in features)\n",
    "        current_price = features.get('close', features.get('current_price'))\n",
    "        if current_price is None:\n",
    "            raise ValueError(\"'close' or 'current_price' must be in features\")\n",
    "        \n",
    "        # Prepare feature vector\n",
    "        feature_cols = self.config['feature_columns']\n",
    "        X = np.array([[features.get(col, 0) for col in feature_cols]])\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        \n",
    "        # Predict for each horizon\n",
    "        predictions = {}\n",
    "        for horizon in self.config['prediction_horizons']:\n",
    "            pred_return = self.models[horizon].predict(X_scaled)[0]\n",
    "            pred_price = current_price * (1 + pred_return)\n",
    "            \n",
    "            predictions[f'day_{horizon}'] = {\n",
    "                'predicted_price': round(pred_price, 3),\n",
    "                'predicted_return': round(pred_return * 100, 2),  # As percentage\n",
    "                'direction': 'UP' if pred_return > 0 else 'DOWN',\n",
    "                'confidence': round(abs(pred_return) * 10, 2)  # Simple confidence proxy\n",
    "            }\n",
    "        \n",
    "        # Overall recommendation\n",
    "        avg_return = np.mean([predictions[f'day_{h}']['predicted_return'] for h in [1,2,3,4,5]])\n",
    "        if avg_return > 1.5:\n",
    "            recommendation = 'BUY'\n",
    "        elif avg_return < -1.5:\n",
    "            recommendation = 'SELL'\n",
    "        else:\n",
    "            recommendation = 'HOLD'\n",
    "        \n",
    "        return {\n",
    "            'stock': stock_name,\n",
    "            'current_price': current_price,\n",
    "            'predictions': predictions,\n",
    "            'recommendation': recommendation,\n",
    "            'avg_5d_return': round(avg_return, 2)\n",
    "        }\n",
    "\n",
    "# Test the predictor\n",
    "print(\"\\nðŸ§ª Testing BVMTPricePredictor...\")\n",
    "\n",
    "# Create sample features (use last row from test set)\n",
    "sample_features = dict(zip(FEATURE_COLS, X_test_scaled[-1]))\n",
    "sample_features['close'] = test_df['close'].iloc[-1]\n",
    "\n",
    "# This would work with saved models:\n",
    "# predictor = BVMTPricePredictor('models/')\n",
    "# result = predictor.predict('SFBT', sample_features)\n",
    "# print(json.dumps(result, indent=2))\n",
    "\n",
    "print(\"âœ… Inference function ready for backend integration!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. ðŸ“‹ Summary & Next Steps\n",
    "\n",
    "### What We Built:\n",
    "1. âœ… Data loading pipeline for BVMT historical data (2016-2025)\n",
    "2. âœ… Comprehensive feature engineering (30+ features)\n",
    "3. âœ… XGBoost models for 1-5 day price prediction\n",
    "4. âœ… Directional accuracy metrics for trading decisions\n",
    "5. âœ… Production-ready inference class\n",
    "\n",
    "### Model Performance:\n",
    "- Check the results summary above\n",
    "- Directional accuracy > 50% = potentially profitable\n",
    "- Focus on directional accuracy in your pitch!\n",
    "\n",
    "### Files to Copy to Your Project:\n",
    "```\n",
    "ml/models/\n",
    "â”œâ”€â”€ price_predictor_1d.json\n",
    "â”œâ”€â”€ price_predictor_2d.json\n",
    "â”œâ”€â”€ price_predictor_3d.json\n",
    "â”œâ”€â”€ price_predictor_4d.json\n",
    "â”œâ”€â”€ price_predictor_5d.json\n",
    "â”œâ”€â”€ feature_scaler.pkl\n",
    "â””â”€â”€ config.json\n",
    "```\n",
    "\n",
    "### Backend Integration:\n",
    "Copy the `BVMTPricePredictor` class to `backend/app/services/prediction.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ðŸŽ‰ Notebook Complete!\n",
    "\n",
    "**Good luck with the hackathon!** ðŸš€\n",
    "\n",
    "**IHEC-CODELAB 2.0 - La Casa del Papel Edition**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
