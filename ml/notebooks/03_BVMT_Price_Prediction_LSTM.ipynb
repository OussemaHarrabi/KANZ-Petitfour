{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ¦ BVMT Stock Price Prediction with LSTM\n",
    "## IHEC-CODELAB 2.0 - Deep Learning Pipeline\n",
    "\n",
    "**Objective:** Predict the next 5 days closing prices for BVMT stocks using LSTM\n",
    "\n",
    "**Model:** LSTM (Long Short-Term Memory) - captures temporal dependencies\n",
    "\n",
    "**Metrics:** RMSE, MAE, Directional Accuracy\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“‹ Table of Contents\n",
    "1. Setup & Data Loading\n",
    "2. Data Exploration & Cleaning\n",
    "3. Feature Engineering\n",
    "4. Sequence Preparation for LSTM\n",
    "5. Model Architecture & Training\n",
    "6. Evaluation & Visualization\n",
    "7. Export for Production"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ðŸ“¦ Setup & Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run in Colab)\n",
    "!pip install -q tensorflow pandas numpy scikit-learn matplotlib seaborn ta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "import gc\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# TensorFlow/Keras imports\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization, Input, Bidirectional\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import mixed_precision\n",
    "\n",
    "# === COLAB GPU OPTIMIZATIONS ===\n",
    "# Enable mixed precision for faster training & lower memory\n",
    "mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "# Enable memory growth to prevent OOM\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    print(f\"âœ… GPU memory growth enabled for {len(gpus)} GPU(s)\")\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('dark_background')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "print(f\"âœ… TensorFlow version: {tf.__version__}\")\n",
    "print(f\"âœ… GPU available: {tf.config.list_physical_devices('GPU')}\")\n",
    "print(f\"âœ… Mixed precision: {mixed_precision.global_policy().name}\")\n",
    "print(\"âœ… Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“ Upload your data files to Colab\n",
    "from google.colab import files\n",
    "uploaded = files.upload()\n",
    "print(f\"âœ… Uploaded {len(uploaded)} files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_txt_file(filepath):\n",
    "    \"\"\"Load 2016-2021 fixed-width text files.\"\"\"\n",
    "    df = pd.read_fwf(\n",
    "        filepath,\n",
    "        skiprows=2,\n",
    "        header=None,\n",
    "        names=['SEANCE', 'GROUPE', 'CODE', 'VALEUR', 'OUVERTURE', 'CLOTURE', \n",
    "               'PLUS_BAS', 'PLUS_HAUT', 'QUANTITE_NEGOCIEE', 'NB_TRANSACTION', 'CAPITAUX', 'IND_RES'],\n",
    "        encoding='latin-1'\n",
    "    )\n",
    "    if 'IND_RES' in df.columns:\n",
    "        df = df.drop('IND_RES', axis=1)\n",
    "    return df\n",
    "\n",
    "def load_csv_file(filepath):\n",
    "    \"\"\"Load 2022-2025 CSV files.\"\"\"\n",
    "    df = pd.read_csv(filepath, sep=';', encoding='utf-8')\n",
    "    df.columns = df.columns.str.strip()\n",
    "    return df\n",
    "\n",
    "def load_all_data():\n",
    "    \"\"\"Load and combine all data files.\"\"\"\n",
    "    all_dfs = []\n",
    "    \n",
    "    for year in range(2016, 2022):\n",
    "        try:\n",
    "            df = load_txt_file(f'histo_cotation_{year}.txt')\n",
    "            df['source_year'] = year\n",
    "            all_dfs.append(df)\n",
    "            print(f\"âœ… Loaded {year}: {len(df)} rows\")\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Could not load {year}: {e}\")\n",
    "    \n",
    "    for year in range(2022, 2026):\n",
    "        try:\n",
    "            df = load_csv_file(f'histo_cotation_{year}.csv')\n",
    "            df['source_year'] = year\n",
    "            all_dfs.append(df)\n",
    "            print(f\"âœ… Loaded {year}: {len(df)} rows\")\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Could not load {year}: {e}\")\n",
    "    \n",
    "    combined = pd.concat(all_dfs, ignore_index=True)\n",
    "    print(f\"\\nðŸ“Š Total combined: {len(combined)} rows\")\n",
    "    return combined\n",
    "\n",
    "raw_data = load_all_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ðŸ” Data Exploration & Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ“‹ Dataset Info:\")\n",
    "print(f\"Shape: {raw_data.shape}\")\n",
    "print(f\"\\nColumns: {raw_data.columns.tolist()}\")\n",
    "raw_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    \"\"\"Clean and standardize the BVMT dataset.\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    def parse_date(x):\n",
    "        if pd.isna(x):\n",
    "            return pd.NaT\n",
    "        x = str(x).strip()\n",
    "        for fmt in ['%d/%m/%Y', '%Y-%m-%d', '%d-%m-%Y']:\n",
    "            try:\n",
    "                return pd.to_datetime(x, format=fmt)\n",
    "            except:\n",
    "                continue\n",
    "        return pd.NaT\n",
    "    \n",
    "    df['date'] = df['SEANCE'].apply(parse_date)\n",
    "    \n",
    "    df['GROUPE'] = pd.to_numeric(df['GROUPE'], errors='coerce')\n",
    "    df = df[df['GROUPE'] == 11].copy()\n",
    "    print(f\"âœ… Filtered to main market: {len(df)} rows\")\n",
    "    \n",
    "    numeric_cols = ['OUVERTURE', 'CLOTURE', 'PLUS_BAS', 'PLUS_HAUT', \n",
    "                    'QUANTITE_NEGOCIEE', 'NB_TRANSACTION', 'CAPITAUX']\n",
    "    for col in numeric_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    \n",
    "    df = df.rename(columns={\n",
    "        'VALEUR': 'stock_name',\n",
    "        'CODE': 'stock_code',\n",
    "        'OUVERTURE': 'open',\n",
    "        'CLOTURE': 'close',\n",
    "        'PLUS_BAS': 'low',\n",
    "        'PLUS_HAUT': 'high',\n",
    "        'QUANTITE_NEGOCIEE': 'volume',\n",
    "        'NB_TRANSACTION': 'transactions',\n",
    "        'CAPITAUX': 'capital'\n",
    "    })\n",
    "    \n",
    "    df = df.dropna(subset=['date'])\n",
    "    df = df[(df['close'] > 0) & df['close'].notna()]\n",
    "    df = df[df['volume'] > 0].copy()\n",
    "    print(f\"âœ… After removing invalid data: {len(df)} rows\")\n",
    "    \n",
    "    df = df.sort_values(['stock_name', 'date']).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "data = clean_data(raw_data)\n",
    "print(f\"\\nðŸ“Š Cleaned dataset shape: {data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select top liquid stocks\n",
    "stock_stats = data.groupby('stock_name').agg({\n",
    "    'volume': ['count', 'sum'],\n",
    "    'close': ['mean', 'std'],\n",
    "    'date': ['min', 'max']\n",
    "}).round(2)\n",
    "\n",
    "stock_stats.columns = ['trading_days', 'total_volume', 'avg_price', 'price_std', 'first_date', 'last_date']\n",
    "stock_stats = stock_stats.sort_values('total_volume', ascending=False)\n",
    "\n",
    "TOP_N_STOCKS = 15\n",
    "selected_stocks = stock_stats.head(TOP_N_STOCKS).index.tolist()\n",
    "data = data[data['stock_name'].isin(selected_stocks)].copy()\n",
    "\n",
    "print(f\"ðŸ“Š Selected {TOP_N_STOCKS} stocks with {len(data)} total records\")\n",
    "print(f\"\\nStocks: {selected_stocks}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ðŸ› ï¸ Feature Engineering\n",
    "\n",
    "Same features as XGBoost for fair comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(df):\n",
    "    \"\"\"Create technical indicators and features for LSTM.\"\"\"\n",
    "    df = df.copy().sort_values('date').reset_index(drop=True)\n",
    "    \n",
    "    # Returns\n",
    "    df['return_1d'] = np.log(df['close'] / df['close'].shift(1))\n",
    "    df['return_5d'] = np.log(df['close'] / df['close'].shift(5))\n",
    "    df['return_10d'] = np.log(df['close'] / df['close'].shift(10))\n",
    "    df['return_20d'] = np.log(df['close'] / df['close'].shift(20))\n",
    "    \n",
    "    # Volatility\n",
    "    df['volatility_5d'] = df['return_1d'].rolling(5).std()\n",
    "    df['volatility_10d'] = df['return_1d'].rolling(10).std()\n",
    "    df['volatility_20d'] = df['return_1d'].rolling(20).std()\n",
    "    \n",
    "    # Moving Averages\n",
    "    for window in [5, 10, 20, 50]:\n",
    "        df[f'sma_{window}'] = df['close'].rolling(window).mean()\n",
    "        df[f'price_to_sma_{window}'] = df['close'] / df[f'sma_{window}']\n",
    "    \n",
    "    # EMA\n",
    "    df['ema_12'] = df['close'].ewm(span=12).mean()\n",
    "    df['ema_26'] = df['close'].ewm(span=26).mean()\n",
    "    df['price_to_ema_12'] = df['close'] / df['ema_12']\n",
    "    \n",
    "    # Volume features\n",
    "    df['volume_sma_5'] = df['volume'].rolling(5).mean()\n",
    "    df['volume_sma_20'] = df['volume'].rolling(20).mean()\n",
    "    df['volume_ratio'] = df['volume'] / df['volume_sma_20']\n",
    "    df['volume_change'] = df['volume'].pct_change()\n",
    "    \n",
    "    # RSI\n",
    "    delta = df['close'].diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
    "    rs = gain / loss\n",
    "    df['rsi_14'] = 100 - (100 / (1 + rs))\n",
    "    \n",
    "    # MACD\n",
    "    df['macd'] = df['ema_12'] - df['ema_26']\n",
    "    df['macd_signal'] = df['macd'].ewm(span=9).mean()\n",
    "    df['macd_hist'] = df['macd'] - df['macd_signal']\n",
    "    \n",
    "    # Bollinger Bands\n",
    "    df['bb_middle'] = df['close'].rolling(20).mean()\n",
    "    df['bb_std'] = df['close'].rolling(20).std()\n",
    "    df['bb_upper'] = df['bb_middle'] + 2 * df['bb_std']\n",
    "    df['bb_lower'] = df['bb_middle'] - 2 * df['bb_std']\n",
    "    df['bb_width'] = (df['bb_upper'] - df['bb_lower']) / df['bb_middle']\n",
    "    df['bb_position'] = (df['close'] - df['bb_lower']) / (df['bb_upper'] - df['bb_lower'])\n",
    "    \n",
    "    # Price range\n",
    "    df['intraday_range'] = (df['high'] - df['low']) / df['close']\n",
    "    df['gap_open'] = (df['open'] - df['close'].shift(1)) / df['close'].shift(1)\n",
    "    \n",
    "    # Time features\n",
    "    df['day_of_week'] = df['date'].dt.dayofweek\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['quarter'] = df['date'].dt.quarter\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply to each stock\n",
    "print(\"ðŸ› ï¸ Creating features...\")\n",
    "featured_data = []\n",
    "for stock in selected_stocks:\n",
    "    stock_df = data[data['stock_name'] == stock].copy()\n",
    "    stock_df = create_features(stock_df)\n",
    "    featured_data.append(stock_df)\n",
    "    print(f\"  âœ… {stock}: {len(stock_df)} rows\")\n",
    "\n",
    "data_with_features = pd.concat(featured_data, ignore_index=True)\n",
    "print(f\"\\nðŸ“Š Total: {data_with_features.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature columns (same as XGBoost)\n",
    "FEATURE_COLS = [\n",
    "    'return_1d', 'return_5d', 'return_10d', 'return_20d',\n",
    "    'volatility_5d', 'volatility_10d', 'volatility_20d',\n",
    "    'price_to_sma_5', 'price_to_sma_10', 'price_to_sma_20', 'price_to_sma_50',\n",
    "    'price_to_ema_12',\n",
    "    'volume_ratio', 'volume_change',\n",
    "    'rsi_14', 'macd', 'macd_signal', 'macd_hist',\n",
    "    'bb_width', 'bb_position',\n",
    "    'intraday_range', 'gap_open',\n",
    "    'day_of_week', 'month', 'quarter'\n",
    "]\n",
    "\n",
    "print(f\"ðŸ“‹ Using {len(FEATURE_COLS)} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ðŸ”„ Sequence Preparation for LSTM\n",
    "\n",
    "LSTM requires sequences of data. We'll use a sliding window approach:\n",
    "- **Input**: Past N days of features\n",
    "- **Output**: Next 5 days returns (Day 1, Day 2, Day 3, Day 4, Day 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM Hyperparameters\n",
    "SEQUENCE_LENGTH = 60  # Look back 60 trading days (~3 months)\n",
    "PREDICTION_HORIZONS = [1, 2, 3, 4, 5]  # Predict 5 days ahead\n",
    "\n",
    "print(f\"âš™ï¸ LSTM Configuration:\")\n",
    "print(f\"  Sequence length: {SEQUENCE_LENGTH} days\")\n",
    "print(f\"  Prediction horizons: {PREDICTION_HORIZONS}\")\n",
    "print(f\"  Features per timestep: {len(FEATURE_COLS)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_targets(df, horizons=[1, 2, 3, 4, 5]):\n",
    "    \"\"\"Create target variables for multi-day prediction.\"\"\"\n",
    "    df = df.copy()\n",
    "    for h in horizons:\n",
    "        df[f'target_return_{h}d'] = (df['close'].shift(-h) - df['close']) / df['close']\n",
    "        df[f'target_price_{h}d'] = df['close'].shift(-h)\n",
    "    return df\n",
    "\n",
    "# Apply targets per stock\n",
    "print(\"ðŸŽ¯ Creating targets...\")\n",
    "final_data = []\n",
    "for stock in selected_stocks:\n",
    "    stock_df = data_with_features[data_with_features['stock_name'] == stock].copy()\n",
    "    stock_df = create_targets(stock_df)\n",
    "    final_data.append(stock_df)\n",
    "\n",
    "df_final = pd.concat(final_data, ignore_index=True)\n",
    "df_final = df_final.dropna()\n",
    "print(f\"ðŸ“Š Final dataset: {df_final.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "def prepare_sequences_per_stock(df, feature_cols, sequence_length, horizons):\n",
    "    \"\"\"\n",
    "    Prepare sequences for LSTM training per stock.\n",
    "    \n",
    "    Returns:\n",
    "        X: (samples, sequence_length, features)\n",
    "        y: (samples, num_horizons) - returns for day 1-5\n",
    "        close_prices: (samples,) - current close price for each sample\n",
    "        dates: (samples,) - dates for each sample\n",
    "    \"\"\"\n",
    "    X_all, y_all = [], []\n",
    "    close_prices_all, dates_all = [], []\n",
    "    \n",
    "    for stock in df['stock_name'].unique():\n",
    "        stock_df = df[df['stock_name'] == stock].sort_values('date').reset_index(drop=True)\n",
    "        \n",
    "        if len(stock_df) < sequence_length + max(horizons):\n",
    "            continue\n",
    "        \n",
    "        # Get features and targets\n",
    "        features = stock_df[feature_cols].values\n",
    "        target_cols = [f'target_return_{h}d' for h in horizons]\n",
    "        targets = stock_df[target_cols].values\n",
    "        close = stock_df['close'].values\n",
    "        dates = stock_df['date'].values\n",
    "        \n",
    "        # Create sequences\n",
    "        for i in range(sequence_length, len(stock_df) - max(horizons)):\n",
    "            X_all.append(features[i-sequence_length:i])\n",
    "            y_all.append(targets[i])\n",
    "            close_prices_all.append(close[i])\n",
    "            dates_all.append(dates[i])\n",
    "    \n",
    "    return np.array(X_all), np.array(y_all), np.array(close_prices_all), np.array(dates_all)\n",
    "\n",
    "# Prepare sequences\n",
    "print(\"ðŸ”„ Preparing sequences...\")\n",
    "X, y, close_prices, dates = prepare_sequences_per_stock(\n",
    "    df_final, FEATURE_COLS, SEQUENCE_LENGTH, PREDICTION_HORIZONS\n",
    ")\n",
    "\n",
    "print(f\"\\nðŸ“Š Sequence shapes:\")\n",
    "print(f\"  X: {X.shape} (samples, timesteps, features)\")\n",
    "print(f\"  y: {y.shape} (samples, horizons)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features\n",
    "# For LSTM, we scale each feature across the entire dataset\n",
    "\n",
    "# Reshape for scaling: (samples * timesteps, features)\n",
    "n_samples, n_timesteps, n_features = X.shape\n",
    "X_flat = X.reshape(-1, n_features)\n",
    "\n",
    "# Fit scaler\n",
    "feature_scaler = StandardScaler()\n",
    "X_flat_scaled = feature_scaler.fit_transform(X_flat)\n",
    "\n",
    "# Reshape back to sequences\n",
    "X_scaled = X_flat_scaled.reshape(n_samples, n_timesteps, n_features)\n",
    "\n",
    "print(f\"âœ… Features scaled: {X_scaled.shape}\")\n",
    "\n",
    "# Scale targets (returns) - optional but can help\n",
    "target_scaler = StandardScaler()\n",
    "y_scaled = target_scaler.fit_transform(y)\n",
    "\n",
    "print(f\"âœ… Targets scaled: {y_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal train/val/test split\n",
    "# Sort by date and split\n",
    "sort_idx = np.argsort(dates)\n",
    "X_scaled = X_scaled[sort_idx]\n",
    "y_scaled = y_scaled[sort_idx]\n",
    "y = y[sort_idx]\n",
    "close_prices = close_prices[sort_idx]\n",
    "dates = dates[sort_idx]\n",
    "\n",
    "n = len(X_scaled)\n",
    "train_end = int(n * 0.7)\n",
    "val_end = int(n * 0.85)\n",
    "\n",
    "X_train, X_val, X_test = X_scaled[:train_end], X_scaled[train_end:val_end], X_scaled[val_end:]\n",
    "y_train, y_val, y_test = y_scaled[:train_end], y_scaled[train_end:val_end], y_scaled[val_end:]\n",
    "y_train_raw, y_val_raw, y_test_raw = y[:train_end], y[train_end:val_end], y[val_end:]\n",
    "close_train, close_val, close_test = close_prices[:train_end], close_prices[train_end:val_end], close_prices[val_end:]\n",
    "dates_train, dates_val, dates_test = dates[:train_end], dates[train_end:val_end], dates[val_end:]\n",
    "\n",
    "print(f\"ðŸ“Š Data Split:\")\n",
    "print(f\"  Train: {len(X_train)} samples\")\n",
    "print(f\"  Val:   {len(X_val)} samples\")\n",
    "print(f\"  Test:  {len(X_test)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ðŸ§  LSTM Model Architecture\n",
    "\n",
    "We'll build a multi-output LSTM that predicts returns for all 5 days simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm_model(input_shape, output_dim, units=[128, 64], dropout=0.2):\n",
    "    \"\"\"\n",
    "    Build LSTM model for multi-horizon stock prediction.\n",
    "    \n",
    "    Architecture:\n",
    "    - Bidirectional LSTM layers (capture forward and backward patterns)\n",
    "    - Batch normalization (stabilize training)\n",
    "    - Dropout (prevent overfitting)\n",
    "    - Dense output layer (predict 5 days simultaneously)\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        # Input layer\n",
    "        Input(shape=input_shape),\n",
    "        \n",
    "        # First Bidirectional LSTM\n",
    "        Bidirectional(LSTM(units[0], return_sequences=True)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(dropout),\n",
    "        \n",
    "        # Second LSTM\n",
    "        LSTM(units[1], return_sequences=False),\n",
    "        BatchNormalization(),\n",
    "        Dropout(dropout),\n",
    "        \n",
    "        # Dense layers\n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(dropout / 2),\n",
    "        \n",
    "        # Output: 5 predictions (day 1-5 returns)\n",
    "        Dense(output_dim, activation='linear')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Build model\n",
    "input_shape = (SEQUENCE_LENGTH, len(FEATURE_COLS))\n",
    "output_dim = len(PREDICTION_HORIZONS)  # 5 days\n",
    "\n",
    "model = build_lstm_model(\n",
    "    input_shape=input_shape,\n",
    "    output_dim=output_dim,\n",
    "    units=[128, 64],\n",
    "    dropout=0.3\n",
    ")\n",
    "\n",
    "# Compile\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='mse',\n",
    "    metrics=['mae']\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=15,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        min_lr=1e-6,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        'best_lstm_model.keras',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        verbose=0\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"âœ… Callbacks configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "print(\"ðŸš‚ Training LSTM model...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=100,\n",
    "    batch_size=64,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nâœ… Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(history.history['loss'], label='Train Loss', color='#ff4444')\n",
    "axes[0].plot(history.history['val_loss'], label='Val Loss', color='#44ff44')\n",
    "axes[0].set_title('Model Loss', color='white')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss (MSE)')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# MAE\n",
    "axes[1].plot(history.history['mae'], label='Train MAE', color='#ff4444')\n",
    "axes[1].plot(history.history['val_mae'], label='Val MAE', color='#44ff44')\n",
    "axes[1].set_title('Model MAE', color='white')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('MAE')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ðŸ“Š Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Predict on test set\n",
    "y_pred_scaled = model.predict(X_test, verbose=0)\n",
    "\n",
    "# Inverse transform to get actual returns\n",
    "y_pred = target_scaler.inverse_transform(y_pred_scaled)\n",
    "y_actual = y_test_raw\n",
    "\n",
    "# Calculate metrics for each horizon\n",
    "print(\"ðŸ“Š Test Set Performance (per prediction horizon):\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "results = {}\n",
    "for i, horizon in enumerate(PREDICTION_HORIZONS):\n",
    "    pred = y_pred[:, i]\n",
    "    actual = y_actual[:, i]\n",
    "    \n",
    "    rmse = np.sqrt(mean_squared_error(actual, pred))\n",
    "    mae = mean_absolute_error(actual, pred)\n",
    "    \n",
    "    # Directional accuracy\n",
    "    dir_acc = np.mean((pred > 0) == (actual > 0))\n",
    "    \n",
    "    results[horizon] = {\n",
    "        'rmse': rmse,\n",
    "        'mae': mae,\n",
    "        'directional_accuracy': dir_acc\n",
    "    }\n",
    "    \n",
    "    print(f\"  Day {horizon}: RMSE={rmse:.6f}, MAE={mae:.6f}, Dir.Acc={dir_acc:.2%}\")\n",
    "\n",
    "# Average metrics\n",
    "avg_rmse = np.mean([r['rmse'] for r in results.values()])\n",
    "avg_mae = np.mean([r['mae'] for r in results.values()])\n",
    "avg_dir_acc = np.mean([r['directional_accuracy'] for r in results.values()])\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nðŸŽ¯ Average: RMSE={avg_rmse:.6f}, MAE={avg_mae:.6f}, Dir.Acc={avg_dir_acc:.2%}\")\n",
    "print(\"\\n(Directional accuracy > 50% is profitable in theory!)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions vs actual\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, horizon in enumerate(PREDICTION_HORIZONS):\n",
    "    ax = axes[i]\n",
    "    pred = y_pred[:, i] * 100  # Convert to percentage\n",
    "    actual = y_actual[:, i] * 100\n",
    "    \n",
    "    ax.scatter(actual, pred, alpha=0.3, s=5, color='#ff4444')\n",
    "    ax.plot([-10, 10], [-10, 10], 'w--', alpha=0.5, label='Perfect prediction')\n",
    "    ax.set_xlabel('Actual Return (%)')\n",
    "    ax.set_ylabel('Predicted Return (%)')\n",
    "    ax.set_title(f'Day {horizon} Prediction\\nDir.Acc: {results[horizon][\"directional_accuracy\"]:.2%}', color='white')\n",
    "    ax.set_xlim(-10, 10)\n",
    "    ax.set_ylim(-10, 10)\n",
    "    ax.grid(alpha=0.3)\n",
    "    ax.axhline(y=0, color='white', linewidth=0.5, alpha=0.5)\n",
    "    ax.axvline(x=0, color='white', linewidth=0.5, alpha=0.5)\n",
    "\n",
    "# Summary in last subplot\n",
    "ax = axes[5]\n",
    "horizons_str = [f'Day {h}' for h in PREDICTION_HORIZONS]\n",
    "dir_accs = [results[h]['directional_accuracy'] * 100 for h in PREDICTION_HORIZONS]\n",
    "bars = ax.bar(horizons_str, dir_accs, color='#ff4444')\n",
    "ax.axhline(y=50, color='white', linestyle='--', label='Random (50%)')\n",
    "ax.set_ylabel('Directional Accuracy (%)')\n",
    "ax.set_title('Directional Accuracy by Horizon', color='white')\n",
    "ax.set_ylim(0, 100)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time series visualization for sample predictions\n",
    "n_samples_to_show = 200\n",
    "\n",
    "fig, axes = plt.subplots(len(PREDICTION_HORIZONS), 1, figsize=(16, 12), sharex=True)\n",
    "\n",
    "for i, horizon in enumerate(PREDICTION_HORIZONS):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    pred = y_pred[-n_samples_to_show:, i] * 100\n",
    "    actual = y_actual[-n_samples_to_show:, i] * 100\n",
    "    \n",
    "    ax.plot(pred, label='Predicted', color='#ff4444', linewidth=1)\n",
    "    ax.plot(actual, label='Actual', color='#44ff44', linewidth=1, alpha=0.7)\n",
    "    ax.axhline(y=0, color='white', linewidth=0.5, alpha=0.5)\n",
    "    ax.set_ylabel(f'Day {horizon} Return (%)')\n",
    "    ax.legend(loc='upper right')\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "axes[0].set_title('LSTM Predictions vs Actual Returns (Last 200 Samples)', color='white', fontsize=14)\n",
    "axes[-1].set_xlabel('Sample Index')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ðŸ’¾ Export for Production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "os.makedirs('models_lstm', exist_ok=True)\n",
    "\n",
    "# Save Keras model\n",
    "model.save('models_lstm/lstm_price_predictor.keras')\n",
    "print(\"âœ… Saved models_lstm/lstm_price_predictor.keras\")\n",
    "\n",
    "# Save scalers\n",
    "joblib.dump(feature_scaler, 'models_lstm/feature_scaler.pkl')\n",
    "joblib.dump(target_scaler, 'models_lstm/target_scaler.pkl')\n",
    "print(\"âœ… Saved scalers\")\n",
    "\n",
    "# Save config\n",
    "config = {\n",
    "    'model_type': 'LSTM',\n",
    "    'sequence_length': SEQUENCE_LENGTH,\n",
    "    'feature_columns': FEATURE_COLS,\n",
    "    'prediction_horizons': PREDICTION_HORIZONS,\n",
    "    'selected_stocks': selected_stocks,\n",
    "    'results': {str(k): {kk: float(vv) for kk, vv in v.items()} for k, v in results.items()},\n",
    "    'avg_directional_accuracy': float(avg_dir_acc),\n",
    "    'created_at': datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "with open('models_lstm/config.json', 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "print(\"âœ… Saved models_lstm/config.json\")\n",
    "\n",
    "print(\"\\nðŸ“¦ All LSTM models exported!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download (Colab)\n",
    "from google.colab import files\n",
    "\n",
    "!zip -r models_lstm.zip models_lstm/\n",
    "files.download('models_lstm.zip')\n",
    "\n",
    "print(\"\\nðŸ“¥ Download complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ðŸ”§ Inference Class (for Backend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === COPY THIS TO YOUR BACKEND ===\n",
    "\n",
    "class LSTMPricePredictor:\n",
    "    \"\"\"\n",
    "    Production LSTM price predictor for BVMT stocks.\n",
    "    \n",
    "    Usage:\n",
    "        predictor = LSTMPricePredictor('models_lstm/')\n",
    "        prediction = predictor.predict('SFBT', historical_features)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_dir: str):\n",
    "        import json\n",
    "        import joblib\n",
    "        from tensorflow import keras\n",
    "        \n",
    "        with open(f'{model_dir}/config.json', 'r') as f:\n",
    "            self.config = json.load(f)\n",
    "        \n",
    "        self.model = keras.models.load_model(f'{model_dir}/lstm_price_predictor.keras')\n",
    "        self.feature_scaler = joblib.load(f'{model_dir}/feature_scaler.pkl')\n",
    "        self.target_scaler = joblib.load(f'{model_dir}/target_scaler.pkl')\n",
    "        \n",
    "        print(f\"âœ… Loaded LSTM model\")\n",
    "    \n",
    "    def predict(self, stock_name: str, historical_data: pd.DataFrame) -> dict:\n",
    "        \"\"\"\n",
    "        Predict next 5 days prices.\n",
    "        \n",
    "        Args:\n",
    "            stock_name: Stock name\n",
    "            historical_data: DataFrame with at least SEQUENCE_LENGTH rows\n",
    "                            and all required feature columns\n",
    "        \n",
    "        Returns:\n",
    "            Prediction dictionary with prices for days 1-5\n",
    "        \"\"\"\n",
    "        import numpy as np\n",
    "        \n",
    "        seq_len = self.config['sequence_length']\n",
    "        feature_cols = self.config['feature_columns']\n",
    "        \n",
    "        if len(historical_data) < seq_len:\n",
    "            raise ValueError(f\"Need at least {seq_len} rows of data\")\n",
    "        \n",
    "        # Get last seq_len rows\n",
    "        recent = historical_data.tail(seq_len)\n",
    "        current_price = recent['close'].iloc[-1]\n",
    "        \n",
    "        # Extract features\n",
    "        X = recent[feature_cols].values\n",
    "        \n",
    "        # Scale\n",
    "        X_scaled = self.feature_scaler.transform(X)\n",
    "        X_input = X_scaled.reshape(1, seq_len, len(feature_cols))\n",
    "        \n",
    "        # Predict\n",
    "        pred_scaled = self.model.predict(X_input, verbose=0)\n",
    "        pred_returns = self.target_scaler.inverse_transform(pred_scaled)[0]\n",
    "        \n",
    "        # Build response\n",
    "        predictions = {}\n",
    "        for i, horizon in enumerate(self.config['prediction_horizons']):\n",
    "            ret = pred_returns[i]\n",
    "            pred_price = current_price * (1 + ret)\n",
    "            \n",
    "            predictions[f'day_{horizon}'] = {\n",
    "                'predicted_price': round(pred_price, 3),\n",
    "                'predicted_return': round(ret * 100, 2),\n",
    "                'direction': 'UP' if ret > 0 else 'DOWN'\n",
    "            }\n",
    "        \n",
    "        avg_return = np.mean(pred_returns) * 100\n",
    "        if avg_return > 1.5:\n",
    "            recommendation = 'BUY'\n",
    "        elif avg_return < -1.5:\n",
    "            recommendation = 'SELL'\n",
    "        else:\n",
    "            recommendation = 'HOLD'\n",
    "        \n",
    "        return {\n",
    "            'stock': stock_name,\n",
    "            'model': 'LSTM',\n",
    "            'current_price': current_price,\n",
    "            'predictions': predictions,\n",
    "            'recommendation': recommendation,\n",
    "            'avg_5d_return': round(avg_return, 2)\n",
    "        }\n",
    "\n",
    "print(\"âœ… LSTMPricePredictor class ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. ðŸ“‹ Summary\n",
    "\n",
    "### Model Architecture:\n",
    "- **Input**: 60-day sequences of 25 features\n",
    "- **Layers**: Bidirectional LSTM(128) â†’ LSTM(64) â†’ Dense(32) â†’ Dense(5)\n",
    "- **Output**: 5 predicted returns (day 1-5)\n",
    "\n",
    "### Key Differences from XGBoost:\n",
    "| Aspect | XGBoost | LSTM |\n",
    "|--------|---------|------|\n",
    "| Input | Single feature vector | Sequence of 60 days |\n",
    "| Training | Fast (~1 min) | Slower (~5-10 min) |\n",
    "| Temporal patterns | Implicit via lagged features | Explicit via memory cells |\n",
    "| Interpretability | Feature importance | Harder to interpret |\n",
    "\n",
    "### Files Generated:\n",
    "```\n",
    "models_lstm/\n",
    "â”œâ”€â”€ lstm_price_predictor.keras\n",
    "â”œâ”€â”€ feature_scaler.pkl\n",
    "â”œâ”€â”€ target_scaler.pkl\n",
    "â””â”€â”€ config.json\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# ðŸŽ‰ LSTM Notebook Complete!\n",
    "\n",
    "**IHEC-CODELAB 2.0**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
